---
title: "Shrinking Cities’ Population Estimation: Night Lights as an Intermediary"
author: "Alex Tan"
output: pdf
bibiliography: 
  html_document: default
  pdf_document: default
---

# Introduction
Night Lights data (Yi et al., 2014) has determined urbanization processes and its’ ability to track urban sprawl expansion, the urban environment, and its’ level of activity. While it may detect urban expansion, but is it an acceptable fit for shrinking cities? 

The NOAA’s National Centers for Environmental Information (NCEI) supports DoD's data archival for the DMSP sensors, which the U.S. Air Force Global Weather Central sends DMSP data to NCEI. Archival operation was established in March of 1992, and began receiving data on a daily basis in September of 1992.

The Defense Meteorological Satellite Program is a collection of two satellites that are solar coordinated, which one satellite is in a dawn - dusk orbit, and the second in a day - night orbit. The DMSP Operational Linescan System (OLS) data sets are utilized for this project. As stated on NOAA’s page, 

“The wide swath widths provide for global coverage four times a day: dawn, day, dusk, night.” 

“With sunlight eliminated, the light intensification makes it possible to detect city lights, gas flares, and fires.”

The population of a city determines its existence with its inhabitant’s activity (lights usage) measurement a good way to detect its’ vibrancy or inactivity. Therefore, Rust Belt cities offer intriguing insights of shrinking cities and its population.


# Data Pulling
The data (Ngdc.noaa.gov, 2017) set obtained for the project can be obtained as below:
https://www.ngdc.noaa.gov/eog/viirs/download_dnb_composites.html

Disclaimer: File Size is VERY HUGE! It is highly recommended that codes be ran locally!

As for the focus of the project, the global cloud free VIIRS Cloud Mask product is selected (VCMCFG). The project will utilize the data for the month of September 2014 and September 2016. Reasons for selecting the data sets:
i) Allowing an even time interval
ii) Allowing changes to be more apparent biennially

The objective of the codes below is to load GeoTIFFs from NOAA (geo-referenced raster file) and MSA shape files from the US Census Bureau. The spatial projection coordinates for both data sets will be synchronized using World Geodetic System 1984 (WGS84) datum.

The codes below, saved as data pull.R script, will pull the data from the relevant directory once data sets have been downloaded:
```{r, echo=TRUE, eval=FALSE,}
#Preliminary Libraries
library(rgdal)

#Set TIF images
##################################################################################################################
##Obtain a list of TIF files, load in the first file in list

### Year 2014 TIF
rast2014=raster(paste("img/092014avg_rade9.tif",sep=""))

### Year 2016 TIF
rast2016=raster(paste("img/092016avg_rade9.tif",sep=""))
  
##Specify WGS84 as the projection of the raster file
wgs84="+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
projection(rast2014)=CRS(wgs84)
projection(rast2016)=CRS(wgs84)
###################################################################################################################

#Set Census Data
####################################################################################################################
##Setting MSA Shapefile
msa2014=readOGR("censusdata/cb_2014_us_cbsa_20m.shp")
msa2016=readOGR("censusdata/cb_2016_us_cbsa_20m.shp")

##Synchronizing projection with TIF Files
projection(msa2014)=CRS(wgs84)
projection(msa2016)=CRS(wgs84)

##Convert MSA names to char
msa_pop2014=read.csv("censusdata/cbsa-est2014-alldata.csv")
msa_pop2014=msa_pop2014[msa_pop2014$LSAD=="Metropolitan Statistical Area",]
msa_pop2014=msa_pop2014[order(msa_pop2014$POPESTIMATE2014),]
msa_pop2014$NAME=as.character(msa_pop2014$NAME)

msa_pop2016=read.csv("censusdata/cbsa-est2016-alldata.csv")
msa_pop2016=msa_pop2016[msa_pop2016$LSAD=="Metropolitan Statistical Area",]
msa_pop2016=msa_pop2016[order(msa_pop2016$POPESTIMATE2016),]
msa_pop2016$NAME=as.character(msa_pop2016$NAME)


```

# Data Wrangling
As for tidying the data set, the project will utilize the libraries as commented in the codes. The data will be pulled from a script file as mentioned above. 

The project has chosen five cities for illustration. Namely, they are, Chicago, Detroit, Flint, MI, Cleveland, OH, and Buffalo. 
```{r, echo=TRUE, eval=FALSE}
#Load libraries
{
  library(doParallel)
  library(foreach)
  library(raster)
  library(sp)
  library(rgdal)
  library(ggmap)
  library(ggplot2)
}
#####################################################################################

#Data
source("code/datapull.R")
#####################################################################################

#Selecting Cities
cities=c("Chicago, IL", "Detroit, MI", "Flint, MI", "Cleveland, OH", "Buffalo, NY")

##Chart Display Parameters
par(mai=c(0,0,0,0),mfrow = c(3,2),bg='#001a4d', bty='n')
######################################################################################


```


# Visualization
The visualization process will use a loop function for both years to geocode and will be assigned with relevant variables. 

The loop function works by utilizing "foreach"" library. It will run the cities that were selected above.

Firstly in the loop function,  the cities will be geocoded and assigned temporarily before merged.

Secondly, the bounding box will be established before being merged with the satellite image.

Thirdly, the referenced codes (Chen, 2017) will visually inspect rasters by color coding raster values on a map using k-means clustering algorithm.

Lastly, the sampled radiance level will be plotted to reveal visually of the night lights in relevant cities.

The geocoded cities coordinates are also saved to prevent frequent queries through the ggpmap library as Google's Term of Policies limits queries.

![2014](plots/Year2014.png)

![2016](plots/Year2016.png)
```{r, echo=TRUE, eval=FALSE}
#Visualizing Cities for Year 2016
coords2016=data.frame()

##Loop
for(i in 1:length(cities)){
  
  ##Coords
  temp_coord=geocode(override_limit = TRUE, cities[i], source = c("google", "dsk"), key = "AIzaSyBRzBKmnnK9vwKDrqSBR6olexFThQNWU-8")
  coords2016=rbind(coords2016,temp_coord)
  
  #Bounding Box
  boundbox=extent(temp_coord$lon - 1, temp_coord$lon + 1,
              temp_coord$lat - 0.25, temp_coord$lat + 0.25)
  
  #Pasting Boundbox to Raster
  rc=crop(rast2016, boundbox)    
  
  ##Rescale brackets
  sampled=as.vector(rc)
  clusters=15
  clust=kmeans(sampled,clusters)$cluster
  combined=as.data.frame(cbind(sampled,clust))
  brk=sort(aggregate(combined[,1], list(combined[,2]), max)[,2])
  
  #Plots
  plot(rc, breaks=brk, col=colorRampPalette(c("#001a4d","#0066FF", "yellow"))(clusters), 
       legend=F,yaxt='n',xaxt='n',frame = F, asp=1.5)
  text(temp_coord$lon ,temp_coord$lat + 0.15,
       substr(cities[i],1,regexpr(",",cities[i])-1), 
       col="white", cex=1.25)
  
  rm(combined)
}

#Saving Geocodes  
write.csv(coords2016,file ="data/coords2016.csv" )
#########################################################################################
#Visualizing Cities for Year 2014
########################################################################################
coords2014=data.frame()

##Loop
for(i in 1:length(cities)){
  
  ##Coords
  temp_coord=geocode(override_limit = TRUE, cities[i], source = c("google", "dsk"), key = "AIzaSyBlAs3JB7y49JPpUWMQvZGdie2ZqWNmZss")
  coords2014=rbind(coords2014,temp_coord)
  
  #Bounding Box
  boundbox=extent(temp_coord$lon - 1, temp_coord$lon + 1,
                  temp_coord$lat - 0.25, temp_coord$lat + 0.25)
  
  #Pasting Boundbox to Raster
  rc=crop(rast2014, boundbox)    
  
  ##Rescale brackets
  sampled=as.vector(rc)
  clusters=15
  clust=kmeans(sampled,clusters)$cluster
  combined=as.data.frame(cbind(sampled,clust))
  brk=sort(aggregate(combined[,1], list(combined[,2]), max)[,2])
  
  #Plots
  plot(rc, breaks=brk, col=colorRampPalette(c("#001a4d","#0066FF", "yellow"))(clusters), 
       legend=F,yaxt='n',xaxt='n',frame = F, asp=1.5)
  text(temp_coord$lon ,temp_coord$lat + 0.15,
       substr(cities[i],1,regexpr(",",cities[i])-1), 
       col="white", cex=1.25)
  
  rm(combined)
}

#Saving Geocodes
write.csv(coords2014,file ="data/coords2014.csv" )
###############################################################################################

```


# Creating Histogram of Satellite Image (Raster Data to Vector Data Conversion)
The codes below referenced (Chen, 2017) from the guidelines given by Department of Commerce will convert GeoTIFFs to vector data. Again, it will use a loop for both years.
```{r, echo=TRUE, eval=FALSE}
##Histogram 2016
histogram=function(shp,rast2016,i){
  
  #Extract one polygon based on index value i
  polygon=shp[i,] #extract one polygon
  extent=extent(polygon) #extract the polygon extent 
  
  #Raster extract
  outer=crop(rast2016, extent) #extract raster by polygon extent
  inner=mask(outer,polygon) #keeps values from raster extract that are within polygon
  
  #Convert cropped raster into a vector
  #Specify coordinates
  coords2016=expand.grid(seq(extent@xmin,extent@xmax,(extent@xmax-extent@xmin)/(ncol(inner)-1)),
                        seq(extent@ymin,extent@ymax,(extent@ymax-extent@ymin)/(nrow(inner)-1)))
  
  #Convert raster into vector
  data=as.vector(inner)
  
  #tidying data into dataframe
  data=cbind(as.character(shp@data$CBSAFP[i]),coords2016, data) 
  colnames(data)=c("GEOID","lon","lat","avg_rad") #note that 
  data=data[!is.na(data$avg_rad),] #keep non-NA values only
  
  return(data)
}

##Histogram 2014
histogram=function(shp,rast2014,i){
  
  #Extract one polygon based on index value i
  polygon=shp[i,] #extract one polygon
  extent=extent(polygon) #extract the polygon extent 
  
  #Raster extract
  outer=crop(rast2014, extent) #extract raster by polygon extent
  inner=mask(outer,polygon) #keeps values from raster extract that are within polygon
  
  #Convert cropped raster into a vector
  #Specify coordinates
  coords2014=expand.grid(seq(extent@xmin,extent@xmax,(extent@xmax-extent@xmin)/(ncol(inner)-1)),
                         seq(extent@ymin,extent@ymax,(extent@ymax-extent@ymin)/(nrow(inner)-1)))
  #Convert raster into vector
  data=as.vector(inner)
  
  #package data in neat dataframe
  data=cbind(as.character(shp@data$CBSAFP[i]),coords2014, data) 
  colnames(data)=c("GEOID","lon","lat","avg_rad") #note that 
  data=data[!is.na(data$avg_rad),] #keep non-NA values only
  
  return(data)
}
```



# Results
In order to obtain the necessary radiance statistics within the geographic boundaries specified, the five cities, the project will utilize two cores, limited by budget capacity, to speed up the extraction process.

Once the extraction is done, it will have the sum of the total night lights with the relevant "GEOID". Then, the process of merging it with the Metropolitan Statistical Area's population statistic can be done.

The process described is performed for both years to obtain the necessary statistics for plotting. Plots are done logarithmically due to large sum size and magnitude.
```{r, echo=TRUE, eval=FALSE}
#Plotting
#Scatterplot with Trendline of "TNL and Population"

##Year 2016
registerDoParallel(cores=2)
extract2016=foreach(i=1:nrow(msa2016@data), .combine=rbind, .packages="raster") %dopar% {
  data=histogram(msa2016,rast2016,i)
  data.frame(GEOID = data$GEOID[1],sum = sum(data$avg_rad))
}
extract2016$GEOID=as.numeric(as.character(extract2016$GEOID))

##Merge data
merge2016=merge(extract2016, msa_pop2016[,c("CBSA","NAME","POPESTIMATE2016")],by.x="GEOID",by.y="CBSA")

colnames(merge2016)=c("GEOID","TNL","MSA","Population")

#Saving Large Extracted Data 2016
write.csv(extract2016,file ="data/extract2016.csv" )

#Plotting
ggplot(merge2016, aes(log(TNL), log(Population))) +
  geom_hex() +
  geom_smooth(aes(color = log(TNL))) +
  labs(title="Total Night Time Light vs Population 2016")
    

#---------------------------------------------------------------------------------------------------
##Year 2014
registerDoParallel(cores=2)
extract2014=foreach(i=1:nrow(msa2014@data), .combine=rbind, .packages="raster") %dopar% {
  data=histogram(msa2014,rast2014,i)
  data.frame(GEOID = data$GEOID[1],sum = sum(data$avg_rad))
}
extract2014$GEOID=as.numeric(as.character(extract2014$GEOID))

#Saving Large Extracted Data 2014  
write.csv(extract2014,file ="data/extract2014.csv" )

##Merge data
merge2014=merge(extract2014, msa_pop2014[,c("CBSA","NAME","POPESTIMATE2014")],by.x="GEOID",by.y="CBSA")

colnames(merge2014)=c("GEOID","TNL","MSA","Population")

#Plotting
ggplot(merge2014, aes(log(TNL), log(Population))) +
  geom_hex() +
  geom_smooth(aes(color = log(TNL))) +
  labs(title="Total Night Time Light vs Population 2014")



```

![2014](plots/PLOT2014.png)

![2016](plots/PLOT2016.png)



# Conclusions
The plotted chart clearly shows a good fit of using the sum of night lights data by DMSP OLS in relation to the population estimates by US Census Bureau, even on shrinking cities. The plot comparison between the two years show the trend line being sticky despite the drop in sum of night lights as the years goes by. 

Future studies with more time intervals and shorter time intervals, provided by better budget for computing capacity, would be aspirational to the better precision. 


# References
Yi, K., Tani, H., Li, Q., Zhang, J., Guo, M., Bao, Y., Wang, X. and Li, J. (2014). Mapping and Evaluating the Urbanization Process in Northeast China Using DMSP/OLS Nighttime Light Data. Sensors, 14(2), pp.3207-3226.

Ngdc.noaa.gov. (2017). NOAA/NGDC - Earth Observation Group - Defense Meteorological Satellite Progam, Boulder. [online] Available at: https://www.ngdc.noaa.gov/eog/viirs/download_dnb_composites.html .

Chen, J. (2017). CommerceDataService/tutorial_viirs_part1. [online] GitHub. Available at: https://github.com/CommerceDataService/tutorial_viirs_part1 .
